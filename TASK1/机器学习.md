# 蒟蒻的机器学习基础小记



## 一、学习模式

### (一) 监督学习

给定数据集，包含输入-标准输出，利用已有的数据进行分析，以对后续任何可能的输入做出一个输出

#### 1.回归 - 预测器

预测连续结果，比如房子大小-房价

预测器就是实现这个过程的机器

#### 2.分类 - 分类器

预测不同种类，有限的可能结果，比如肿瘤大小-良性恶性预测，表情图片-表情识别

分类器就是实现这个过程的机器



### (二) 无监督学习

给定数据集，不包含示例正确答案，分析以找到其中的某种有趣的模式or结构，不要求预测输出数据结果

* 聚类算法	把未标记的数据放在不同集群中，比如新闻分类推送
* 异常检测    比如诈骗识别

* 降维            大数据集-->小数据集





## 二、数据集

数据集可以随机打乱后划分为训练集和测试集

### (一) 训练集

用来训练神经网络的一部分数据集，利用训练集的输入特征和输出目标来训练一个算法模型

### (二) 测试集

用来检测神经网络算法准确率的一部分数据集





## 三、神经网络性能评价指标

数据样本分为正样本&负样本，判断对为真，判断错为假（可以借助核酸阴阳性理解一下）

所以样本最后分为真正$TP$，真负$TN$，假正（实负）$FP$，假负（实正）$FN$

$P=TP+FN$, $N=TN+FP$

### (一) 准确率Accuracy

全局判断对了多少   $acc=\frac{TP+TN}{P+N}=\frac{TP+TN}{TP+TN+FP+FN}$

但是准确率不是越高越好，因为数据分布不一样(重要程度)，而准确率是针对全局的

### (二) 错误率Error rate

跟准确率相对

### (三) 灵敏度Sensitive&特效度Specificity

相当于局部的准确率

$Sensitive=\frac{TP}{P}=\frac{TP}{TP+FN}$    正样本

$Specificity=\frac{TN}{N}=\frac{TP}{TN+FP}$   负样本

### (四) 精确率/精度Precision

真正占预测正的比例

$Precision=\frac{TP}{TP+FP}$

### (五) 综合评价指标F-measure

灵敏度和精度会有矛盾，则加权平均来调和

### (六) 另：过拟合

过拟合就是模型 <u>**过于刁钻**</u> 贴合样本数据

样本中存在一种无关紧要的特征，但是模型把它视为重要特征一起学习，最后就会过拟合

数学上来看就是训练集loss小，测试集loss大，损失小泛化差



## 四、基础神经网络结构

### (一) 从神经元到神经网络

<img src="https://s1.328888.xyz/2022/10/03/P186d.png" style="zoom: 67%;" /> 

#### 1.神经元

圆圈代表一个神经元（节点），一个神经元大概相当于一个运算器，对输入神经元的值进行一定运算然后得到输出

输入输出层的节点数由一般由数据集决定，个数是固定的，隐藏层节点数由自己设置

#### 2.输入层

输入层输入的就是数据集dataset的特征值(data)，神经网络的输入需要转化为数值

为了后续的计算，我们要先把它转化为tensor张量

#### 3.隐藏层

隐藏层对输入层进行加权和激活

#### 4.输出层

得到dataset的预测结果





### (二) 回归模型

第n层的任一节点就是对(n-1)层所有节点的加权运算和激活，第(n-1)层节点对第n层不同节点的权重不同

一个节点的运算一般就是加权运算+激活函数

#### 1.权重

比如用肿瘤大小、颜色、形状等特征来预测分类是良性还是恶性，每一个特征对目标的重要性不同，也就是权重不同

#### 2.激活函数

加权运算得到的模型始终是线性的，但是我们的问题一般不会是简单的线性关系，所以需要用一个非线性函数也就是激活函数使得模型从线性结构变复杂，经过激活的函数可以接近我们想要的任何函数

常见激活函数举例：

* $Sigmoid$函数，把R压缩到 (0,1)
* $ReLU$函数，$ReLU=max(0,x)$





### (三) 简单算法实现

#### 1.数据处理

数据集划分为训练集和测试集，输入对应x_train, x_test, 输出对应y_train, y_test

用torch把数据张量化，x~i~ , y~i~可以变成向量形式

##### (1) 特征工程

比如，有些特征可以合并来看，比如地皮长宽，乘起来作为新特征对地价预测更有意义

不仅限于合并，特征工程就是对原有特征进行一定处理，获得更有价值的新特征

##### (2) 向量化

$\vec{w}=[w_1~w_2~w_3]$

$\vec{x}=[x_1~x_2~x_3]$

$f_{\vec{w},b}(\vec{x})=\vec{w}\cdot\vec{x}+b$

把x~i~ 向量化，后续加权运算（w~i~也向量化了）就可以变成矩阵乘法，可以并行运算，利用torch的运算功能，而不用用到for循环一个一个算，提高了运算效率

##### (3) 独热码

在分类问题中，先把 y 类别名称转化为数值0,1,2...

如果设置输出层为一个节点，那么经过模型运算后会输出一个值，这个值一般不会恰好等于类别值，也就不好判定最终属于什么类别

所以我们设置输出层节点数和类别数一样，最后用$Softmax$函数处理输出，得到和为1的概率分布

比如得到结果为[0.1, 0.3, 0.6]那么也就是类别2的概率最大

为了和这个结果对应，我们把类别数值用独热码转换，类别0对应[1, 0, 0]，类别1对应[0, 1, 0]， 以此类推，这就是独热码



#### 2.训练过程

设置训练次数，然后把测试集放进神经网络，循环经历这个过程：



##### (1) 前向传播

在NET类中，初始化每层节点数，然后进行加权运算、激活函数运算，就能实现前一层到下一层的向前传播

##### (2) 损失函数计算

针对单个样本

前向传播最后得到输出预测值，损失函数则可以计算衡量预测值和真实值的差距（不是简单的两值相减）

loss函数具体有很多种模型，适用场景不同

训练模型要朝着loss减小的方向进行

* 代价函数cost，针对所有样本，并且还包括CPU占用情况等等

##### (3) 用梯度下降法进行权重更新


$$
\begin{align}
temp\_w&=\,w\,-\,lr\frac{\partial}{\partial w}loss(w,b)\\\\
temp\_b&=\,b\,-\,lr\frac{\partial}{\partial b}loss(w,b)\\\\
w&=temp\_w\\\\
b&=temp\_b\\
\end{align}
$$




###### ① 权重更新

训练过程中我们要减小loss，就需要对权重进行更新，让权重更贴近真实权重

###### ② 梯度下降 学习率

loss下降的快慢程度相当于梯度（对loss求导）

想象在群山中，每次权重更新就来到某一点，我们的目的是尽快走到山谷（loss尽可能小），那就应该往最陡的方向走（梯度减小的方向），并且每次迈的步长要适中，太小要走很久，太大很容易走过头（步长相当于学习率，学习率要适中）

每次权重更新都要再计算一次loss和梯度

梯度下降得到局部最优解

###### ③ 优化器

torch提供了一些具体的优化器，对传统的梯度下降进行了优化，引导权重更新和反向传播，可以直接调用

##### (4) 反向传播

一次性计算全局梯度复杂度太高，所以从输出层往输入层逐层传递loss计算梯度更新权重，即反向传播







## 五、卷积

### (一) 引例

卷积的运算就是加权叠加。

引用经典巴掌打脸例子，影响=影响产生*影响消除

<img src="https://s1.328888.xyz/2022/10/05/PWzew.png" style="zoom:50%;" /> 



### (二) 卷积定义

两个变量在一定范围内相乘求和

$(f*g)(x)=f(x)*g(x)=\int_{-\infty}^{+\infty}f(t)g(x-t)$

### (三) 卷积在图像处理中的运用

卷积是对<u>**一块像素区域**</u>处理，提取局部特征（而不是所有像素）

利用一个奇数n*n卷积核对图像矩阵滑动处理得到卷积图，可以进行平滑、锐化等操作
